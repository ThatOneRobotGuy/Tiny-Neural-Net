{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017b9623-5212-464b-813a-aa746ecda244",
   "metadata": {},
   "source": [
    "# Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180c1e1-322b-4896-a967-4c07f45bfb47",
   "metadata": {},
   "source": [
    "Besides some small mistakes (Using weights = for update instead of += by accident) it was\n",
    "flawless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a213b2-edcb-42f3-9e51-09262991434f",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb583d-41ab-4bda-a47b-b596834bba47",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69357d84-b626-43f2-82ee-4b059ea769e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339d873-d932-4ee9-b011-d2e036c46eb7",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cfdbbe5-7873-4eb0-bd3b-9b41f8f921ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights0 = 2 * np.random.rand(3,4) # 3 inputs into 4 hidden layer neurons\n",
    "weights1 = 2 * np.random.rand(4,1) # 4 inputs into 1 output layer neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147f7e6-0125-4350-8801-1f05ccc30f14",
   "metadata": {},
   "source": [
    "## Define your activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db0c5734-b474-4bdb-a02a-ef2add390a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "# If deriv = True, x must be output from sigmoid to return true derivative\n",
    "def Sigmoid(x, deriv = False):\n",
    "    if(deriv):\n",
    "        return x * (1-x)\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bc946-3ac5-4443-9392-d3c99ed10b50",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66bd6c0b-30a1-4d6d-b85c-462ee8e725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = X\n",
    "l1 = Sigmoid(np.dot(l0, weights0)) # Each row corresponds to one data point, each column 1 neuron\n",
    "l2 = Sigmoid(np.dot(l1, weights1)) # Same as 11, each row represents the estimated probablity of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50cb04-6489-4c47-8597-1267da50dc6c",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b9c8e7-1d32-46d7-907c-d2920ca1161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error = y - l2 # Global error of estimate\n",
    "l2_delta = l2_error * Sigmoid(l2, deriv = True) # Error scaled inversely with confidence\n",
    "\n",
    "# Error contribution of each l1\n",
    "# Each row represents one data point\n",
    "# Each column represents one l1 neuron\n",
    "# Each value is the error contribution of that neuron to that data point estimate\n",
    "l1_error = np.dot(l2_delta, weights1.T)\n",
    "l1_delta = l1_error * Sigmoid(l1, deriv=True) # Error contribution scaled inversely with confidence\n",
    "\n",
    "weights1 = np.dot(l1.T, l2_delta) # Change by sum of (Input * Scaled_Error)\n",
    "weights0 = np.dot(l0.T, l1_delta) # Change by sum of (Input * Scaled-Error-Contribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43d1b3-1827-4725-ac2a-374371439202",
   "metadata": {},
   "source": [
    "# Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37bcb53-bb15-4688-8675-cf5c447e3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4964100319027255\n",
      "0.008584525653247157\n",
      "0.005789459862507806\n",
      "0.004629176776769983\n",
      "0.003958765280273646\n",
      "0.0035101225678616736\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "weights0 = 2 * np.random.rand(3,4) - 1 # 3 inputs into 4 hidden layer neurons\n",
    "weights1 = 2 * np.random.rand(4,1) - 1 # 4 inputs into 1 output layer neuron\n",
    "\n",
    "# Sigmoid function\n",
    "# If deriv = True, x must be output from sigmoid to return true derivative\n",
    "def Sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "for j in range(60000):\n",
    "    l0 = X\n",
    "    l1 = Sigmoid(np.dot(l0, weights0)) # Each row corresponds to one data point, each column 1 neuron\n",
    "    l2 = Sigmoid(np.dot(l1, weights1)) # Same as 11, each row represents the estimated probablity of 1\n",
    "    \n",
    "    l2_error = y - l2 # Global error of estimate\n",
    "    if (j % 10000 == 0):\n",
    "        print(np.mean(np.abs(l2_error)))\n",
    "    \n",
    "    l2_delta = l2_error * Sigmoid(l2, deriv = True) # Error scaled inversely with confidence\n",
    "    \n",
    "    # Error contribution of each l1\n",
    "    # Each row represents one data point\n",
    "    # Each column represents one l1 neuron\n",
    "    # Each value is the error contribution of that neuron to that data point estimate\n",
    "    l1_error = np.dot(l2_delta, weights1.T)\n",
    "    l1_delta = l1_error * Sigmoid(l1, deriv=True) # Error contribution scaled inversely with confidence\n",
    "    \n",
    "    weights1 += np.dot(l1.T, l2_delta) # Change by sum of (Input * Scaled_Error)\n",
    "    weights0 += np.dot(l0.T, l1_delta) # Change by sum of (Input * Scaled-Error-Contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a77949-8430-44d9-8666-54ca5013a6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
